{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对中文新闻题目进行分类\n",
    "\n",
    "本示例使用头条客户端抓取的新闻题目分类，演示如何用Amazon Sagemaker内置算法BlazingText对新闻标题进行分类。\n",
    "\n",
    "原数据集下载地址：https://github.com/skdjfla/toutiao-text-classfication-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import json\n",
    "import boto3\n",
    "from random import shuffle\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role) # This is the role that SageMaker would use to leverage AWS resources (S3, CloudWatch) on your behalf\n",
    "\n",
    "bucket = sess.default_bucket() # Replace with your own bucket name if needed\n",
    "print(bucket)\n",
    "prefix = 'blazingtext/supervised/toutiao' #Replace with the prefix under which you want to store the data if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: jieba in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.42.1)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.2b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -i https://pypi.tuna.tsinghua.edu.cn/simple jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'100': '民生', '101': '文化', '102': '娱乐', '103': '体育', '104': '财经', '106': '房产', '107': '汽车', '108': '教育', '109': '科技', '110': '军事', '112': '旅游', '113': '国际', '114': '证券', '115': '农业', '116': '电竞'}\n"
     ]
    }
   ],
   "source": [
    "index_to_label = {} \n",
    "with open(\"classes.txt\") as f:\n",
    "    for i,label in enumerate(f.readlines()):\n",
    "        ll = label.strip().split(',')\n",
    "        index_to_label[ll[0]] = ll[1]\n",
    "print(index_to_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "!unzip toutiao_cat_data.txt.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['__label__财经', '偿付能力', '逼近', '监管', '红线', '弘康', '人寿', '补血', '涉险', '过关'], ['__label__国际', '越南', '拥有', '110', '余万', '军队', '为何', '越南', '军队', '最', '擅长', '布置', '竹签', '阵'], ['__label__娱乐', '网友', '偶遇', '钟丽缇', '在', '妇产科', '检查'], ['__label__汽车', '是', '做梦', '吗', '网传', '只花', '几十元', '爱车', '隔音', '效果', '堪比', '豪车'], ['__label__电竞', '衣之国', '今日', '不', '删档', '测试', '满足', '你', '的', '美丽', '梦想']]\n"
     ]
    }
   ],
   "source": [
    "file  = 'toutiao_cat_data.txt'#'toutiao_cat_data.txt'\n",
    "with open(file) as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "labels = []\n",
    "for line in lines:\n",
    "    label = []\n",
    "    line = line.split('_!_')\n",
    "    label_code = index_to_label[line[1]]\n",
    "    label.append('__label__' + label_code)\n",
    "    line[3] = re.sub(r\"[\\s+\\.\\!\\/_,$%^*()?;；:-【】+\\\"\\']+|[+——一！，;:：。？、~@#￥%……&*（）]+\", \"\", line[3])\n",
    "    label.extend(jieba.cut(line[3],cut_all=False))\n",
    "#     print(label)\n",
    "#     label = '__label__' + label_code + ' ' + artical_title\n",
    "    labels.append(label)\n",
    "    \n",
    "shuffle(labels)\n",
    "print(labels[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'blazingtext/toutiao'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_train_data = labels[0:int(len(labels)*0.8)]\n",
    "t_validation_data = labels[int(len(labels)*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['__label__财经', '偿付能力', '逼近', '监管', '红线', '弘康', '人寿', '补血', '涉险', '过关'],\n",
       " ['__label__国际',\n",
       "  '越南',\n",
       "  '拥有',\n",
       "  '110',\n",
       "  '余万',\n",
       "  '军队',\n",
       "  '为何',\n",
       "  '越南',\n",
       "  '军队',\n",
       "  '最',\n",
       "  '擅长',\n",
       "  '布置',\n",
       "  '竹签',\n",
       "  '阵'],\n",
       " ['__label__娱乐', '网友', '偶遇', '钟丽缇', '在', '妇产科', '检查'],\n",
       " ['__label__汽车',\n",
       "  '是',\n",
       "  '做梦',\n",
       "  '吗',\n",
       "  '网传',\n",
       "  '只花',\n",
       "  '几十元',\n",
       "  '爱车',\n",
       "  '隔音',\n",
       "  '效果',\n",
       "  '堪比',\n",
       "  '豪车'],\n",
       " ['__label__电竞', '衣之国', '今日', '不', '删档', '测试', '满足', '你', '的', '美丽', '梦想'],\n",
       " ['__label__房产', '房屋买卖', '合同', '签署', '后', '卖方', '要求', '加价', '怎么办'],\n",
       " ['__label__科技',\n",
       "  '支付宝',\n",
       "  '出新招',\n",
       "  '余额',\n",
       "  '宝',\n",
       "  '迎来',\n",
       "  '新',\n",
       "  '变化',\n",
       "  '马云',\n",
       "  '这下',\n",
       "  '又',\n",
       "  '要',\n",
       "  '成功',\n",
       "  '了'],\n",
       " ['__label__电竞',\n",
       "  '西游',\n",
       "  '释厄传',\n",
       "  '看着',\n",
       "  '蜘蛛',\n",
       "  '王',\n",
       "  '缓缓',\n",
       "  '升起',\n",
       "  '我',\n",
       "  '就',\n",
       "  '知道',\n",
       "  '少不了',\n",
       "  '顿',\n",
       "  '揍',\n",
       "  '了'],\n",
       " ['__label__教育', '为什么', '要', '考研', '考研', '有', '什么', '意义'],\n",
       " ['__label__文化',\n",
       "  '中国',\n",
       "  '颜体',\n",
       "  '书法',\n",
       "  '特展',\n",
       "  '第三届',\n",
       "  '宋璟',\n",
       "  '碑',\n",
       "  '颜体',\n",
       "  '书法展',\n",
       "  '特邀',\n",
       "  '作品',\n",
       "  '及',\n",
       "  '入围',\n",
       "  '作品'],\n",
       " ['__label__农业',\n",
       "  '勉县',\n",
       "  '老道',\n",
       "  '寺镇',\n",
       "  '张家湾',\n",
       "  '村三变',\n",
       "  '改革',\n",
       "  '实现',\n",
       "  '村',\n",
       "  '集体',\n",
       "  '收入',\n",
       "  '零',\n",
       "  '突破'],\n",
       " ['__label__教育', '孩子', '读高二', '每次', '回家', '就', '玩', '电脑游戏', '怎么办'],\n",
       " ['__label__体育',\n",
       "  '今晚',\n",
       "  '亚洲',\n",
       "  '100',\n",
       "  '米',\n",
       "  '飞人',\n",
       "  '苏炳添',\n",
       "  '能否',\n",
       "  '延续',\n",
       "  '出色',\n",
       "  '的',\n",
       "  '状态',\n",
       "  '再次',\n",
       "  '冲破',\n",
       "  '10',\n",
       "  '秒',\n",
       "  '大关']]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_train_data[0:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "t_train_file = 'tt.train'\n",
    "t_validation_file = 'tt.validation'\n",
    "\n",
    "with open(t_train_file, 'w') as csvoutfile:\n",
    "    csv_writer = csv.writer(csvoutfile, delimiter=' ', lineterminator='\\n')\n",
    "    csv_writer.writerows(t_train_data)\n",
    "    \n",
    "with open(t_validation_file, 'w') as csvoutfile:\n",
    "    csv_writer = csv.writer(csvoutfile, delimiter=' ', lineterminator='\\n')\n",
    "    csv_writer.writerows(t_validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 285 ms, sys: 91.5 ms, total: 377 ms\n",
      "Wall time: 731 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "t_train_channel = prefix + '/train'\n",
    "t_validation_channel = prefix + '/validation'\n",
    "\n",
    "sess.upload_data(path='tt.train', bucket=bucket, key_prefix=t_train_channel)\n",
    "sess.upload_data(path='tt.validation', bucket=bucket, key_prefix=t_validation_channel)\n",
    "\n",
    "s3_train_data = 's3://{}/{}'.format(bucket, t_train_channel)\n",
    "s3_validation_data = 's3://{}/{}'.format(bucket, t_validation_channel)\n",
    "\n",
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_name = boto3.Session().region_name\n",
    "container = sagemaker.amazon.amazon_estimator.get_image_uri(region_name, \"blazingtext\", \"latest\")\n",
    "print('Using SageMaker BlazingText container: {} ({})'.format(container, region_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_bt_model = sagemaker.estimator.Estimator(container,\n",
    "                                         role, \n",
    "                                         train_instance_count=1, \n",
    "                                         train_instance_type='ml.c4.4xlarge',\n",
    "                                         train_volume_size = 30,\n",
    "                                         train_max_run = 360000,\n",
    "                                         input_mode= 'File',\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=sess)\n",
    "t_bt_model.set_hyperparameters(mode=\"supervised\",\n",
    "                            epochs=10,\n",
    "                            min_count=2,\n",
    "                            learning_rate=0.05,\n",
    "                            vector_dim=10,\n",
    "                            early_stopping=True,\n",
    "                            patience=4,\n",
    "                            min_epochs=5,\n",
    "                            word_ngrams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_train_data = sagemaker.inputs.s3_input(s3_train_data, distribution='FullyReplicated', \n",
    "                        content_type='text/plain', s3_data_type='S3Prefix')\n",
    "t_validation_data = sagemaker.inputs.s3_input(s3_validation_data, distribution='FullyReplicated', \n",
    "                             content_type='text/plain', s3_data_type='S3Prefix')\n",
    "t_data_channels = {'train': t_train_data, 'validation': t_validation_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-23 07:15:43 Starting - Starting the training job...\n",
      "2020-05-23 07:15:55 Starting - Launching requested ML instances......\n",
      "2020-05-23 07:16:56 Starting - Preparing the instances for training......\n",
      "2020-05-23 07:18:03 Downloading - Downloading input data...\n",
      "2020-05-23 07:18:44 Training - Training image download completed. Training in progress.\n",
      "2020-05-23 07:18:44 Uploading - Uploading generated training model\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[05/23/2020 07:18:29 WARNING 140690757523264] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m[05/23/2020 07:18:29 WARNING 140690757523264] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m[05/23/2020 07:18:29 INFO 140690757523264] nvidia-smi took: 0.0252668857574 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[05/23/2020 07:18:29 INFO 140690757523264] Running single machine CPU BlazingText training using supervised mode.\u001b[0m\n",
      "\u001b[34m[05/23/2020 07:18:29 INFO 140690757523264] Processing /opt/ml/input/data/train/tt.train . File size: 24 MB\u001b[0m\n",
      "\u001b[34m[05/23/2020 07:18:29 INFO 140690757523264] Processing /opt/ml/input/data/validation/tt.validation . File size: 6 MB\u001b[0m\n",
      "\u001b[34mRead 3M words\u001b[0m\n",
      "\u001b[34mNumber of words:  81004\u001b[0m\n",
      "\u001b[34mLoading validation data from /opt/ml/input/data/validation/tt.validation\u001b[0m\n",
      "\u001b[34mLoaded validation data.\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 3\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0312  Progress: 37.57%  Million Words/sec: 21.53 #####\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0285  Progress: 43.09%  Million Words/sec: 21.57 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 4\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0257  Progress: 48.70%  Million Words/sec: 21.64 #####\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0229  Progress: 54.25%  Million Words/sec: 21.68 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 5\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.860671\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0189  Progress: 62.23%  Million Words/sec: 19.17 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 6\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.860958\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0153  Progress: 69.41%  Million Words/sec: 17.40 #####\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0126  Progress: 74.81%  Million Words/sec: 17.63 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 7\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.861886\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0091  Progress: 81.86%  Million Words/sec: 16.07 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 8\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.861833\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 1 epochs.\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0040  Progress: 92.08%  Million Words/sec: 15.57 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 9\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.862578\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0008  Progress: 98.50%  Million Words/sec: 14.64 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 10\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.863362\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0000  Progress: 100.00%  Million Words/sec: 13.91 #####\u001b[0m\n",
      "\u001b[34mTraining finished.\u001b[0m\n",
      "\u001b[34mAverage throughput in Million words/sec: 13.91\u001b[0m\n",
      "\u001b[34mTotal training time in seconds: 2.85\n",
      "\u001b[0m\n",
      "\u001b[34m#train_accuracy: 0.9934\u001b[0m\n",
      "\u001b[34mNumber of train examples: 306150\n",
      "\u001b[0m\n",
      "\u001b[34m#validation_accuracy: 0.8634\u001b[0m\n",
      "\u001b[34mNumber of validation examples: 76538\u001b[0m\n",
      "\n",
      "2020-05-23 07:19:01 Completed - Training job completed\n",
      "Training seconds: 58\n",
      "Billable seconds: 58\n"
     ]
    }
   ],
   "source": [
    "t_bt_model.fit(inputs=t_data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using already existing model: blazingtext-2020-05-23-07-15-43-787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "t_text_classifier = t_bt_model.deploy(initial_instance_count = 1,instance_type = 'ml.t2.medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"prob\": [\n",
      "      1.0\n",
      "    ],\n",
      "    \"label\": [\n",
      "      \"__label__\\u5a31\\u4e50\"\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['__label__娱乐']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = \"邓超加入春晚\"#\"宝马推出新车型/亚马逊云计算Q1营收过百亿/美国航空母舰开往伊朗波斯湾/北京迎来黄金周小高峰/C罗纳尔多还是梅西\"\n",
    "\n",
    "# using the same nltk tokenizer that we used during data preparation for training\n",
    "tokenized_sentences = [' '.join(jieba.cut(sentences,cut_all=False))]\n",
    "\n",
    "payload = {\"instances\" : tokenized_sentences}\n",
    "\n",
    "# payload = {\"instances\" : tokenized_sentences,\n",
    "#           \"configuration\": {\"k\": 2}}\n",
    "\n",
    "t_response = t_text_classifier.predict(json.dumps(payload))\n",
    "\n",
    "t_predictions = json.loads(t_response)\n",
    "print(json.dumps(t_predictions, indent=2))\n",
    "t_predictions[0]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
